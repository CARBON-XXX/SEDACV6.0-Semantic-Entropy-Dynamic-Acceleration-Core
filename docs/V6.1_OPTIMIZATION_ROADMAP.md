# SEDAC V6.1 Optimization Roadmap

## Overview

This document outlines the optimization strategy for SEDAC V6.1, covering architecture refactoring, algorithm improvements, and performance enhancements.

---

## Completed Optimizations

### A. Modular Refactoring ✅

**Problem**: Code duplication between `patch_vllm_surgical.py` and `patch_vllm_surgical_v61.py`, scattered core logic.

**Solution**: Created standardized library structure:

```
sedac/
├── core/
│   ├── cascade_controller.py      # Unified cascade control
│   ├── probe_inference.py         # Probe inference with JIT support
│   ├── exit_strategy.py           # Hard/Soft/Adaptive exit strategies
│   └── confidence_fusion.py       # Multi-modal confidence fusion
├── calibration/
│   └── adaptive_threshold.py      # DualMetricCalibration
├── integrations/
│   └── vllm_patcher.py            # Unified V6.0/V6.1 patcher
├── metrics/
│   └── collector.py               # SEDACMetricsCollector
└── tests/
    └── test_suite.py              # Comprehensive test framework
```

**Benefits**:
- 30-40% code reduction
- Isolated unit testing
- Easy to add new exit strategies

---

### B. Rust-Python Interop Optimization ✅

**Problem**: Data transfer overhead, GIL contention in high-throughput scenarios.

**Solution**: Enhanced `sedac-core` Rust crate with:

```rust
// New components
pub struct AsyncBatchProcessor { ... }  // Async batch processing
pub struct TensorBuffer { ... }         // Zero-copy data transfer
fn batch_confidence_accumulate(...);    // SIMD-optimized accumulation
```

**Benefits**:
- 15-25% performance improvement
- Larger batch size support
- Reduced per-token latency

---

### C. Confidence Accumulation Improvements ✅

**Original formula**:
```
C_i = C_{i-1} × γ + conf_i × w_i
```

**New features**:

1. **Adaptive Decay** (`adaptive_confidence_decay`):
   ```python
   γ_t = γ_base × (1 - uncertainty) × (1 + historical_success_bonus)
   ```

2. **Multi-Modal Fusion** (`EnhancedConfidenceAccumulator`):
   - Entropy probe (primary)
   - Perplexity estimation
   - Attention entropy
   - Token consistency

3. **Soft Exit Scaling** (`soft_exit_mlp_scaling`):
   ```python
   scale = 1 / (1 + exp(-k * (confidence_ratio - 0.5)))
   ```

---

### D. Dual-Metric Calibration ✅

**Problem**: Calibration ignores quality metrics, no drift detection.

**Solution**: `DualMetricCalibration` class with:

```python
class DualMetricCalibration:
    - ppl_tolerance: float = 0.01      # Max 1% PPL increase
    - throughput_target: float = 1.1   # 10% improvement target
    - drift_threshold: float = 0.1     # KL divergence for drift
    
    def update_threshold(self, layer_idx, new_threshold, metrics):
        # 1. Quality check (PPL bound)
        # 2. Performance check (throughput)
        # 3. Safe bounds enforcement
        # 4. EMA smoothing
```

**Benefits**:
- Automatic quality protection
- Online drift detection
- Stable production deployment

---

### E. Performance Optimizations ✅

1. **JIT Compilation**: `ProbeManager` with `torch.jit.script`
2. **CUDA Graph**: `setup_cuda_graph()` for fixed-batch inference
3. **LRU Cache**: Feature caching for calibration
4. **Parallel Batch**: Rayon-based parallel evaluation

---

### F. Monitoring & Observability ✅

`SEDACMetricsCollector` provides:

- Per-layer exit statistics
- Token-level decision logging
- Confidence/risk distributions
- Prometheus metrics export

```python
collector.record_token_decision(
    token_id=42,
    layer_idx=14,
    risk_score=0.8,
    threshold=1.0,
    accumulated_confidence=0.65,
    should_exit=True,
    soft_exit_ratio=0.7,
    latency_ms=5.2,
)

report = collector.get_layer_analysis()
prometheus_output = collector.export_prometheus_metrics()
```

---

### G. Comprehensive Test Framework ✅

`ComprehensiveTestSuite` with:

| Test | Description | Metric |
|------|-------------|--------|
| `QualityTest` | PPL preservation | PPL increase < 1% |
| `RobustnessTest` | Noise tolerance | Consistency > 90% |
| `PerformanceTest` | Latency SLO | P99 < threshold |
| `DistributionShiftTest` | Domain robustness | Variance < 0.2 |

---

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                         SEDAC V6.1                              │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │
│  │ ProbeManager│  │ Cascade     │  │ DualMetric              │  │
│  │ (JIT/CUDA)  │→ │ Controller  │→ │ Calibration             │  │
│  └─────────────┘  └─────────────┘  └─────────────────────────┘  │
│         ↓                ↓                     ↓                │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │
│  │ Confidence  │  │ Exit        │  │ Metrics                 │  │
│  │ Fusion      │  │ Strategy    │  │ Collector               │  │
│  └─────────────┘  └─────────────┘  └─────────────────────────┘  │
├─────────────────────────────────────────────────────────────────┤
│                    sedac-core (Rust)                            │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │
│  │ AsyncBatch  │  │ Tensor      │  │ batch_confidence_       │  │
│  │ Processor   │  │ Buffer      │  │ accumulate (SIMD)       │  │
│  └─────────────┘  └─────────────┘  └─────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Priority Matrix

| Priority | Item | Status | Expected Benefit |
|:--------:|------|:------:|------------------|
| P0 | Modular Refactoring | ✅ | Code quality ↑, Maintenance ↓ |
| P0 | Dual-Metric Calibration | ✅ | Quality protection, Stability ↑ |
| P1 | Rust Async Processing | ✅ | 15-25% performance ↑ |
| P1 | Multi-Modal Fusion | ✅ | Misjudgment ↓, Quality ↑ |
| P2 | Performance Optimization | ✅ | 20-30% probe speedup |
| P2 | Monitoring System | ✅ | Diagnosability ↑ |
| P2 | Test Framework | ✅ | Release confidence ↑ |

---

## Usage Examples

### Basic Usage

```python
from sedac.core import CascadeController, CascadeConfig, LayerConfig
from sedac.calibration import DualMetricCalibration
from sedac.metrics import SEDACMetricsCollector

# Configure cascade
config = CascadeConfig(
    layer_configs=[
        LayerConfig(layer_idx=7, target_exit_rate=0.2, initial_threshold=0.8),
        LayerConfig(layer_idx=14, target_exit_rate=0.5, initial_threshold=1.0),
        LayerConfig(layer_idx=21, target_exit_rate=0.8, initial_threshold=1.2),
    ],
    confidence_decay=0.9,
    exit_strategy="soft",
)

controller = CascadeController(config)
metrics = SEDACMetricsCollector(layer_indices=(7, 14, 21))
```

### Multi-Modal Confidence

```python
from sedac.core.confidence_fusion import (
    EnhancedConfidenceAccumulator,
    EntropyProbeEstimator,
    PerplexityEstimator,
)

accumulator = EnhancedConfidenceAccumulator(
    hidden_dim=2048,
    use_gated_fusion=True,
)

accumulator.add_estimator(EntropyProbeEstimator(probe, threshold=1.0), weight=0.6)
accumulator.add_estimator(PerplexityEstimator(2048, vocab_size=32000), weight=0.4)

confidence = accumulator.compute_confidence(hidden_states, layer_idx=14)
```

### Running Tests

```python
from sedac.tests import ComprehensiveTestSuite, TestConfig

suite = ComprehensiveTestSuite(config=TestConfig(
    ppl_tolerance=0.01,
    latency_slo_ms=50.0,
))
suite.add_default_tests()

results = suite.run_all(
    baseline_model=baseline_fn,
    sedac_model=sedac_fn,
    test_data=data,
)

suite.print_report(results)
```

---

## Future Work

1. **Learned Fusion Weights**: End-to-end training of confidence weights
2. **Adaptive Decay Scheduling**: Dynamic γ based on sequence difficulty
3. **Token-Type Routing**: Different cascade paths for different token types
4. **Speculative Integration**: Combine with speculative decoding
